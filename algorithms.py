import numpy as np, yaml, neo, quantities as pq, elephant.conversion as conv, os
from CN2Simulator.motif_gen import *
from scipy import sparse
from itertools import combinations, groupby


def neurons_to_number(neurons, max_neuron, k_tuple):
    number = neurons[0] * pow(max_neuron, k_tuple)
    for k_neurons in range(1, k_tuple + 1):
        number += neurons[k_neurons] * pow(max_neuron, k_tuple - k_neurons)
    return number


def neurons_number_to_neurons(number, max_neuron, k_tuple):
    q, mod = divmod(number, pow(max_neuron, k_tuple))
    neurons = [q]
    for k_neurons in range(1, k_tuple + 1):
        q, mod = divmod(mod, pow(max_neuron, k_tuple - k_neurons))
        neurons.append(q)
    return tuple(sorted(neurons))


def neurons_number_to_unsorted_neurons(number, max_neuron, k_tuple):
    q, mod = divmod(number, pow(max_neuron, k_tuple))
    neurons = [q]
    for k_neurons in range(1, k_tuple + 1):
        q, mod = divmod(mod, pow(max_neuron, k_tuple - k_neurons))
        neurons.append(q)
    return neurons


def data(motif_type, NIDs, recording_time, background_firing_rate, motif_firing_rate, probabilistic_participation, temporal_jitter, time_warping, motif_neurons_numbers, motif_max_lags, motif_max_spikes, bin_size, seed):
    # Synthetic data generated by SimulMotif
    # Load simulation parameters
    with open("params.yaml") as f:
        params = yaml.load(f, Loader=yaml.FullLoader)

    params["NIDs"] = NIDs
    params["recording"]["recording_time"] = recording_time
    params["background"]["firing_rate"] = [background_firing_rate, background_firing_rate]

    if probabilistic_participation > 1:
        params["noise"]["probabilistic_participation"] = 1 - 1 / probabilistic_participation
    params["noise"]["temporal_jitter"] = temporal_jitter
    params["noise"]["time_warping"] = time_warping

    params[f"motif_type_{motif_type}"]["firing_rate"] = motif_firing_rate
    params[f"motif_type_{motif_type}"]["neurons"] = motif_neurons_numbers[0]
    params[f"motif_type_{motif_type}"]["motifs"] = motif_neurons_numbers[1]
    if motif_type > 1:
        params[f"motif_type_{motif_type}"]["max_lags"] = motif_max_lags
        winlen = math.ceil((motif_neurons_numbers[0] - 1) * motif_max_lags / bin_size)
        if motif_type == 3:
            params[f"motif_type_{motif_type}"]["max_spikes"] = motif_max_spikes
    else:
        winlen = round(math.ceil(100.0 / bin_size))

    # Genereate non-motif activity
    # spike_time: list containing every spikes
    # spike_time_motif: list containing spikes induced by motifs
    spike_time, spike_time_motif = non_motif_gen(params, seed=6*seed)

    # Generate motif activity
    motif_gen(spike_time, spike_time_motif, motif_type, params, seed=6*seed+motif_type)
    spiketrains = [neo.core.SpikeTrain(np.clip(np.asarray(x) * 1000, 0, recording_time * 1000) * pq.ms, units='ms', t_stop=recording_time * 1000 * pq.ms) for x in spike_time]
    binary_matrix = conv.BinnedSpikeTrain(spiketrains, bin_size=bin_size * pq.ms, tolerance=None).to_sparse_bool_array().tocoo(copy=False)
    _build_context(binary_matrix, motif_type, NIDs, recording_time, background_firing_rate, probabilistic_participation, temporal_jitter, time_warping, motif_firing_rate, motif_neurons_numbers[0], motif_neurons_numbers[1], motif_max_lags, motif_max_spikes, bin_size, winlen, seed)

    binary_matrix = conv.BinnedSpikeTrain(spiketrains, bin_size=bin_size * pq.ms, tolerance=None).to_bool_array()
    os.makedirs(f"./MIPER/txt/{motif_type}", exist_ok=True)
    f = open(f"./MIPER/txt/{motif_type}/{motif_type}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_seed_{seed}.txt", 'w')
    for time in range(binary_matrix.shape[1]):
        if np.where(binary_matrix[:, time])[0].size > 0:
            f.write(f"{np.where(binary_matrix[:, time])[0][0]}")
            for event in np.where(binary_matrix[:, time])[0][1:]:
                f.write(f" {event}")
        f.write("\n")
    f.close()


def _build_context(binary_matrix, motif_type, NIDs, recording_time, background_firing_rate, probabilistic_participation, temporal_jitter, time_warping, motif_firing_rate, motif_neurons, motif_numbers, motif_max_lags, motif_max_spikes, bin_size, winlen, seed):
    """
    Building the context given a matrix (number of trains x number of bins) of
    binned spike trains

    Parameters
    ----------
    binary_matrix : sparse.coo_matrix
        Binary matrix containing the binned spike trains
    winlen : int
        Length of the bin_size used to bin the spiketrains

    Returns
    -------
    context : list of tuple
        List of tuples containing one object (window position idx) and one of
        the correspondent spikes idx (bin idx * neuron idx)
    transactions : list
        List of all transactions, each element of the list contains the
        attributes of the corresponding object.
    rel_matrix : sparse.coo_matrix
        A binary matrix with shape (number of windows,
        winlen*len(spiketrains)). Each row corresponds to a window (order
        according to their position in time).
        Each column corresponds to one bin and one neuron and it is 0 if no
        spikes or 1 if one or more spikes occurred in that bin for that
        particular neuron.
        E.g. the entry [0,0] of this matrix corresponds to the first bin of the
        first window position for the first neuron, the entry [0,winlen] to the
        first bin of the first window position for the second neuron.
    """
    # Initialization of the outputs
    context = []
    transactions = []
    num_neurons, num_bins = binary_matrix.shape
    indices = np.argsort(binary_matrix.col)
    binary_matrix.row = binary_matrix.row[indices]
    binary_matrix.col = binary_matrix.col[indices]
    # out of all window positions
    # get all non-empty first bins
    unique_cols, unique_col_idx = np.unique(
        binary_matrix.col, return_index=True)
    unique_col_idx = np.concatenate((unique_col_idx, [len(binary_matrix.col)]))
    windows_row = []
    windows_col = []
    # all non-empty bins are starting positions for windows
    for idx, window_idx in enumerate(unique_cols):
        # find the end of the current window in unique_cols
        end_of_window = np.searchsorted(unique_cols, window_idx + winlen)
        # loop over all non-empty bins in the current window
        for rel_idx, col in enumerate(unique_cols[idx:end_of_window]):
            # get all occurrences of the current col in binary_matrix.col
            spike_indices_in_window = np.arange(
                unique_col_idx[idx + rel_idx],
                unique_col_idx[idx + rel_idx + 1])
            # get the binary_matrix.row entries matching the current col
            # prepare the row of rel_matrix matching the current window
            # spikes are indexed as (neuron_id * winlen + bin_id)
            windows_col.extend(
                binary_matrix.row[spike_indices_in_window] * winlen
                + (col - window_idx))
            windows_row.extend([window_idx] * len(spike_indices_in_window))
    # Shape of the rel_matrix:
    # (total number of bins,
    #  number of bins in one window * number of neurons)
    rel_matrix = sparse.coo_matrix(
        (np.ones((len(windows_col)), dtype=bool),
         (windows_row, windows_col)),
        shape=(num_bins, winlen * num_neurons),
        dtype=bool).A
    # Array containing all the possible attributes (each spike is indexed by
    # a number equal to neu idx*winlen + bin_idx)
    attributes = np.asarray(
        [s * winlen + t for s in range(binary_matrix.shape[0])
         for t in range(winlen)])
    # Building context and rel_matrix
    # Looping all the window positions w
    os.makedirs(f"./txt/{motif_type}", exist_ok=True)
    f = open(f"./txt/{motif_type}/{motif_type}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons}_{motif_numbers}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_seed_{seed}.txt", 'w')
    for window in unique_cols:
        # spikes in the current window
        times = rel_matrix[window]
        # adding to the context the window positions and the correspondent
        # attributes (spike idx) (fast_fca input)
        current_transactions = []
        for transaction in sorted(attributes[times], key=lambda x: (x % winlen, x // winlen)):
            context.append((window, transaction))
            current_transactions.append(transaction)
            f.write(f"{transaction} ")
        # appending to the transactions spike idx (fast_fca input) of the
        # current window (fpgrowth input)
        transactions.append(current_transactions)
        f.write("\n")
    f.close()
    # Return context and rel_matrix
    return context, transactions, rel_matrix


def read_ndcg_rc(motif_type, NIDs, recording_time, background_firing_rate, motif_firing_rate, probabilistic_participation, epsilon, temporal_jitter, time_warping, motif_neurons_numbers, motif_max_lags, motif_max_spikes, bin_size, time_interval, option, seed, top_K_list):
    # Synthetic data generated by SimulMotif
    # Load simulation parameters
    with open("params.yaml") as f:
        params = yaml.load(f, Loader=yaml.FullLoader)

    params["NIDs"] = NIDs
    params["recording"]["recording_time"] = recording_time
    params["background"]["firing_rate"] = [background_firing_rate, background_firing_rate]

    if probabilistic_participation > 1:
        params["noise"]["probabilistic_participation"] = 1 - 1 / probabilistic_participation

    params["noise"]["temporal_jitter"] = temporal_jitter
    params["noise"]["time_warping"] = time_warping

    params[f"motif_type_{motif_type}"]["firing_rate"] = motif_firing_rate
    params[f"motif_type_{motif_type}"]["neurons"] = motif_neurons_numbers[0]
    params[f"motif_type_{motif_type}"]["motifs"] = motif_neurons_numbers[1]
    if motif_type > 1:
        params[f"motif_type_{motif_type}"]["max_lags"] = motif_max_lags
        winlen = math.ceil((motif_neurons_numbers[0] - 1) * motif_max_lags / bin_size)
        if motif_type == 3:
            params[f"motif_type_{motif_type}"]["max_spikes"] = motif_max_spikes
        interlen = math.ceil(motif_max_lags / bin_size)
    else:
        winlen = round(math.ceil(100.0 / bin_size))
        interlen = winlen

    if os.path.exists(f"./TSPs/{motif_type}_{option}/{motif_type}_{option}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_ep_{epsilon}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_interlen_{interlen}_inter_{time_interval}_seed_{seed}_{motif_neurons_numbers[0]}.txt"):
        os.makedirs(f"./txt/{motif_type}_{option}", exist_ok=True)
        os.makedirs(f"./figure/{motif_type}_{option}", exist_ok=True)

        # Genereate non-motif activity
        # spike_time: list containing every spikes
        # spike_time_motif: list containing spikes induced by motifs
        spike_time, spike_time_motif = non_motif_gen(params, seed=6*seed)

        # Generate motif activity
        gts = motif_gen(spike_time, spike_time_motif, motif_type, params, seed=6*seed+motif_type)

        motif_neuron = []  # motif neurons
        motif_neuron_num = []  # number of neurons in each motif

        for gt in gts:
            NID = gt['NIDs']
            if motif_type == 3:
                lags_dict = {lag: i_lag for i_lag, lags_i in enumerate(gt['lags']) for lag in lags_i}
                motif_neuron_ = np.asarray([key for key, grp in groupby([NID[lags_dict[lag]] for lag in sorted(lags_dict.keys())])])
                motif_neuron.append(motif_neuron_)
                motif_neuron_num.append(motif_neuron_.size)
            else:
                motif_neuron.append(NID)
                motif_neuron_num.append(NID.size)

        motif_neuron = np.asarray(motif_neuron, dtype=object)
        motif_neuron_num = np.asarray(motif_neuron_num)

        motif_neuron_k = motif_neuron[motif_neuron_num >= motif_neurons_numbers[0]]  # neurons in motif of size larger than k
        condition_positive_set = set(neurons_to_number(sorted(motif_neuron_k_combination), NIDs, motif_neurons_numbers[0] - 1) for motif_neuron_k_i in motif_neuron_k for motif_neuron_k_combination in combinations(motif_neuron_k_i, motif_neurons_numbers[0]))
        condition_positive = len(condition_positive_set)

        neurons_number_list = []
        leverage_list = []

        for line in open(f"./TSPs/{motif_type}_{option}/{motif_type}_{option}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_ep_{epsilon}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_interlen_{interlen}_inter_{time_interval}_seed_{seed}_{motif_neurons_numbers[0]}.txt", 'r'):
            line_split = line.rstrip('\n').split()
            neurons_number_list.append(neurons_to_number(neurons_number_to_neurons(int(line_split[0]), NIDs, motif_neurons_numbers[0] - 1), NIDs, motif_neurons_numbers[0] - 1))
            leverage_list.append(float(line_split[2]))

        neurons_number_list = np.asarray(neurons_number_list)
        leverage_list = np.asarray(leverage_list)
        ind_leverage = np.argsort(leverage_list)[::-1]

        neurons_number_top_K_leverage = []
        neurons_number_top_K_leverage_set = set()
        leverage_number_top_K_leverage = -100
        len_neurons_number_top_K_leverage = 0
        top_K_list[0] = motif_neurons_numbers[1]
        for top_K in top_K_list:
            if len_neurons_number_top_K_leverage < top_K:
                prev_len_neurons_number_top_K_leverage = len_neurons_number_top_K_leverage
                for ind_pval in ind_leverage[prev_len_neurons_number_top_K_leverage:]:
                    neurons_number = neurons_number_list[ind_pval]
                    if neurons_number in neurons_number_top_K_leverage_set:
                        continue
                    leverage = leverage_list[ind_pval]
                    if leverage == leverage_number_top_K_leverage:
                        neurons_number_top_K_leverage[-1].append(neurons_number)
                        len_neurons_number_top_K_leverage += 1
                        continue
                    if len_neurons_number_top_K_leverage >= top_K:
                        break
                    if neurons_number_top_K_leverage:
                        len_1 = len(neurons_number_top_K_leverage[-1])
                        for _ in range(len_1 - 1):
                            neurons_number_top_K_leverage.append([])
                    neurons_number_top_K_leverage.append([neurons_number])
                    neurons_number_top_K_leverage_set.add(neurons_number)
                    leverage_number_top_K_leverage = leverage
                    len_neurons_number_top_K_leverage += 1

            dcg = 0.0
            true_positive = 0.0
            for i, neuron_numbers in enumerate(neurons_number_top_K_leverage):
                len_neuron_numbers = len(neuron_numbers)
                if len_neuron_numbers > 0:
                    true_positive_ratio = sum(1.0 for neuron_number in neuron_numbers if neuron_number in condition_positive_set) / len_neuron_numbers
                    dcg_ = 0.0
                    true_positive_ = 0.0
                    for j in range(i, min(i + len_neuron_numbers, top_K)):
                        dcg_ += 1.0 / np.log2(j + 2)
                        true_positive_ += 1.0
                    dcg += true_positive_ratio * dcg_
                    true_positive += true_positive_ratio * true_positive_

            idcg = sum((1.0 / np.log2(i + 2) for i in range(min(condition_positive, top_K))))
            ndcg = min(dcg / idcg, 1.0)

            rc = min(true_positive / min(condition_positive, top_K), 1.0)

            f_ndcg = open(f"./txt/{motif_type}_{option}_ndcg_.csv", 'a')
            f_ndcg.write(f'{motif_type},{NIDs},{recording_time},{background_firing_rate:.2f},{probabilistic_participation:.1f},{epsilon},{temporal_jitter},{time_warping},{motif_firing_rate:.2f},{motif_neurons_numbers[0]},{motif_neurons_numbers[1]},{motif_max_lags},{motif_max_spikes},{bin_size},{winlen},{interlen},{time_interval},{motif_neurons_numbers[0]},{top_K},{ndcg},{seed}\n')
            f_ndcg.close()

            f_rc = open(f"./txt/{motif_type}_{option}_rc_.csv", 'a')
            f_rc.write(f'{motif_type},{NIDs},{recording_time},{background_firing_rate:.2f},{probabilistic_participation:.1f},{epsilon},{temporal_jitter},{time_warping},{motif_firing_rate:.2f},{motif_neurons_numbers[0]},{motif_neurons_numbers[1]},{motif_max_lags},{motif_max_spikes},{bin_size},{winlen},{interlen},{time_interval},{motif_neurons_numbers[0]},{top_K},{rc},{seed}\n')
            f_rc.close()


def read_ndcg_rc_exp(motif_type, NIDs, recording_time, background_firing_rate, motif_firing_rate, probabilistic_participation, temporal_jitter, time_warping, motif_neurons_numbers, motif_max_lags, motif_max_spikes, bin_size, time_interval, option, seed, top_K_list, i_exp, memory):
    # Synthetic data generated by SimulMotif
    # Load simulation parameters
    with open("params.yaml") as f:
        params = yaml.load(f, Loader=yaml.FullLoader)

    params["NIDs"] = NIDs
    params["recording"]["recording_time"] = recording_time
    params["background"]["firing_rate"] = [background_firing_rate, background_firing_rate]

    if probabilistic_participation > 1:
        params["noise"]["probabilistic_participation"] = 1 - 1 / probabilistic_participation

    params["noise"]["temporal_jitter"] = temporal_jitter
    params["noise"]["time_warping"] = time_warping

    params[f"motif_type_{motif_type}"]["firing_rate"] = motif_firing_rate
    params[f"motif_type_{motif_type}"]["neurons"] = motif_neurons_numbers[0]
    params[f"motif_type_{motif_type}"]["motifs"] = motif_neurons_numbers[1]
    params[f"motif_type_{motif_type}"]["max_lags"] = motif_max_lags
    winlen = math.ceil((motif_neurons_numbers[0] - 1) * motif_max_lags / bin_size)
    interlen = math.ceil(motif_max_lags / bin_size)

    denom = recording_time * 1000.0 / bin_size

    if os.path.exists(f"./TSPs/{motif_type}_{option}{memory}_/{motif_type}_{option}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_interlen_{interlen}_inter_{time_interval}_seed_{seed}_{motif_neurons_numbers[0]}.txt"):
        # Genereate non-motif activity
        # spike_time: list containing every spikes
        # spike_time_motif: list containing spikes induced by motifs
        spike_time, spike_time_motif = non_motif_gen(params, seed=6*seed)

        # Generate motif activity
        gts = motif_gen(spike_time, spike_time_motif, motif_type, params, seed=6*seed+motif_type)

        motif_neuron = []  # motif neurons
        motif_neuron_num = []  # number of neurons in each motif

        for gt in gts:
            NID = gt['NIDs']
            if motif_type == 3:
                lags_dict = {lag: i_lag for i_lag, lags_i in enumerate(gt['lags']) for lag in lags_i}
                motif_neuron_ = np.asarray([key for key, grp in groupby([NID[lags_dict[lag]] for lag in sorted(lags_dict.keys())])])
                motif_neuron.append(motif_neuron_)
                motif_neuron_num.append(motif_neuron_.size)
            else:
                motif_neuron.append(NID)
                motif_neuron_num.append(NID.size)

        motif_neuron = np.asarray(motif_neuron, dtype=object)
        motif_neuron_num = np.asarray(motif_neuron_num)

        motif_neuron_k = motif_neuron[motif_neuron_num >= motif_neurons_numbers[0]]  # neurons in motif of size larger than k
        condition_positive_set = set(neurons_to_number(sorted(motif_neuron_k_combination), NIDs, motif_neurons_numbers[0] - 1) for motif_neuron_k_i in motif_neuron_k for motif_neuron_k_combination in combinations(motif_neuron_k_i, motif_neurons_numbers[0]))
        condition_positive = len(condition_positive_set)

        neurons_number_list = []
        exp_list = []

        dict_item_count = dict()
        if os.path.exists(f"./txt/{motif_type}/{motif_type}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_seed_{seed}.txt"):
            for line in open(f"./txt/{motif_type}/{motif_type}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_seed_{seed}.txt", 'r'):
                for item_ in line.rstrip('\n').split():
                    if int(item_) % winlen == 0:
                        item = int(item_) // winlen
                        if item in dict_item_count:
                            dict_item_count[item] += 1
                        else:
                            dict_item_count[item] = 1

        for line in open(f"./TSPs/{motif_type}_{option}{memory}_/{motif_type}_{option}_NIDs_{NIDs}_time_{recording_time}_bg_{background_firing_rate:.2f}_pp_{probabilistic_participation:.1f}_tj_{temporal_jitter}_tw_{time_warping}_mt_{motif_firing_rate:.2f}_{motif_neurons_numbers[0]}_{motif_neurons_numbers[1]}_{motif_max_lags}_{motif_max_spikes}_bin_{bin_size}_winlen_{winlen}_interlen_{interlen}_inter_{time_interval}_seed_{seed}_{motif_neurons_numbers[0]}.txt", 'r'):
            line_split = line.rstrip('\n').split()
            neurons = neurons_number_to_neurons(int(line_split[0]), NIDs, motif_neurons_numbers[0] - 1)
            neurons_number_list.append(neurons_to_number(neurons, NIDs, motif_neurons_numbers[0] - 1))
            if "over" in option:
                exp_list.append(int(line_split[1]) - denom * np.prod([dict_item_count[neuron] / denom for neuron in neurons]))
            else:
                exp_list.append(int(line_split[1]) - denom * np.prod([dict_item_count[neuron] / denom for neuron in neurons]) * pow(2, motif_neurons_numbers[0] - 1))

        neurons_number_list = np.asarray(neurons_number_list)
        exp_list = np.asarray(exp_list)
        ind_exp = np.argsort(exp_list)[::-1]

        neurons_number_top_K_exp = []
        neurons_number_top_K_exp_set = set()
        exp_number_top_K_exp = -100
        len_neurons_number_top_K_exp = 0
        top_K_list[0] = motif_neurons_numbers[1]
        for top_K in top_K_list:
            if len_neurons_number_top_K_exp < top_K:
                prev_len_neurons_number_top_K_exp = len_neurons_number_top_K_exp
                for ind_pval in ind_exp[prev_len_neurons_number_top_K_exp:]:
                    neurons_number = neurons_number_list[ind_pval]
                    if neurons_number in neurons_number_top_K_exp_set:
                        continue
                    exp = exp_list[ind_pval]
                    if exp == exp_number_top_K_exp:
                        neurons_number_top_K_exp[-1].append(neurons_number)
                        len_neurons_number_top_K_exp += 1
                        continue
                    if len_neurons_number_top_K_exp >= top_K:
                        break
                    if neurons_number_top_K_exp:
                        len_1 = len(neurons_number_top_K_exp[-1])
                        for _ in range(len_1 - 1):
                            neurons_number_top_K_exp.append([])
                    neurons_number_top_K_exp.append([neurons_number])
                    neurons_number_top_K_exp_set.add(neurons_number)
                    exp_number_top_K_exp = exp
                    len_neurons_number_top_K_exp += 1

            dcg = 0.0
            true_positive = 0.0
            for i, neuron_numbers in enumerate(neurons_number_top_K_exp):
                len_neuron_numbers = len(neuron_numbers)
                if len_neuron_numbers > 0:
                    true_positive_ratio = sum(1.0 for neuron_number in neuron_numbers if neuron_number in condition_positive_set) / len_neuron_numbers
                    dcg_ = 0.0
                    true_positive_ = 0.0
                    for j in range(i, min(i + len_neuron_numbers, top_K)):
                        dcg_ += 1.0 / np.log2(j + 2)
                        true_positive_ += 1.0
                    dcg += true_positive_ratio * dcg_
                    true_positive += true_positive_ratio * true_positive_

            idcg = sum((1.0 / np.log2(i + 2) for i in range(min(condition_positive, top_K))))
            ndcg = min(dcg / idcg, 1.0)

            rc = min(true_positive / min(condition_positive, top_K), 1.0)

            f_ndcg = open(f"./txt/{motif_type}_{option}_ndcg_exp_{i_exp}{memory}.csv", 'a')
            f_ndcg.write(f'{motif_type},{NIDs},{recording_time},{background_firing_rate:.2f},{probabilistic_participation:.1f},{temporal_jitter},{time_warping},{motif_firing_rate:.2f},{motif_neurons_numbers[0]},{motif_neurons_numbers[1]},{motif_max_lags},{motif_max_spikes},{bin_size},{winlen},{interlen},{time_interval},{motif_neurons_numbers[0]},{top_K},{ndcg},{seed}\n')
            f_ndcg.close()

            f_rc = open(f"./txt/{motif_type}_{option}_rc_exp_{i_exp}{memory}.csv", 'a')
            f_rc.write(f'{motif_type},{NIDs},{recording_time},{background_firing_rate:.2f},{probabilistic_participation:.1f},{temporal_jitter},{time_warping},{motif_firing_rate:.2f},{motif_neurons_numbers[0]},{motif_neurons_numbers[1]},{motif_max_lags},{motif_max_spikes},{bin_size},{winlen},{interlen},{time_interval},{motif_neurons_numbers[0]},{top_K},{rc},{seed}\n')
            f_rc.close()